data:
  train:
    target: ldm. data.mri.MRIVolumePairedDataset
    params: 
      source_path: "/content/dataset/ULF-v1/ULF-v1Tr"
      target_path: "/content/dataset/HF/HFTr"
      volume_size: [128, 256, 256]  # D, H, W
      load_as_volume: true  # Load entire 3D volumes
    
    train:
      target: ldm.data.mri.MRIPairedDataset  # Or your custom dataset class
      params:
        source_path: "/content/dataset/ULF-v1/ULF-v1Tr"
        target_path: "/content/dataset/HF/HFTr"
        size: 256  # Or your image resolution
        random_crop: true
        flip_prob: 0.5  # Data augmentation
        
    validation:
      target: ldm. data.mri.MRIPairedDataset
      params:
        source_path: "/content/dataset/ULF-v1/ULF-v1Val"
        target_path: "/content/dataset/HF/HFVal"
        size: 256
        random_crop: false  # No augmentation for validation
        flip_prob: 0.0

    test:
      target: ldm. data.mri.MRIPairedDataset
      params:
        source_path: "/content/dataset/ULF-v1/ULF-v1Val"
        target_path: "/content/dataset/HF/HFVal"
        size: 256
        random_crop: false  # No augmentation for test
        flip_prob: 0.0
model:
  target: ldm.models.diffusion.ddpm. LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    
    conditioning_key: "concat"  # For paired image-to-image translation
    
    image_size: 32  # Latent space size (256/8 for VQ-f8 autoencoder)
    channels: 3  # Or 1 for grayscale MRI
    
    unet_config:
      target:  ldm.modules.diffusionmodules.unet3d.UNet3DModel  # Custom 3D UNet
      params:
        image_size: [32, 32, 32]  # Depth, Height, Width in latent space
        in_channels: 2  # Source + noise (grayscale)
        out_channels: 1
        dims: 3  # 3D convolutions
        model_channels: 192
        attention_resolutions: [1, 2, 4, 8]
        num_res_blocks: 2
        channel_mult: [1, 2, 3, 5]
        num_heads: 8
        
    first_stage_config:
      target: ldm.models.autoencoder.VQModelInterface
      params:
        embed_dim: 3  # Or 1 for grayscale
        n_embed: 8192
        ckpt_path: "models/first_stage_models/vq-f8/model.ckpt"  # Pretrained autoencoder
        
    cond_stage_config:  "__is_first_stage__"  # Use input image as conditioning

lightning:
  callbacks:
    image_logger:
      target: main.ImageLogger
      params:
        batch_frequency: 500
        max_images: 8
        increase_log_steps: false
        
  trainer:
    max_epochs: 100
    gradient_clip_val: 1.0
    accumulate_grad_batches: 1
    check_val_every_n_epoch: 1